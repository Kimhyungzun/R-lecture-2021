accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
precision <- precision + t[2,2]/(t[2,1]+t[2,2])
recall <- recall + t[2,2]/(t[1,2]+t[2,2])
}
sv_avg_acc <- accuracy / k
sv_avg_prec <- precision / k
sv_avg_rec <- recall / k
# K-Nearest Neighbor
accuracy <- 0
precision <- 0
recall <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
pred <- knn(data_train[, 2:4], data_test[, 2:4],
data_train$admit, k=5)
t <- table(pred, data_test$admit)
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
precision <- precision + t[2,2]/(t[2,1]+t[2,2])
recall <- recall + t[2,2]/(t[1,2]+t[2,2])
}
kn_avg_acc <- accuracy / k
kn_avg_prec <- precision / k
kn_avg_rec <- recall / k
sprintf('결정트리: 정확도=%f, 정밀도=%f, 재현율=%f',
dt_avg_acc, dt_avg_prec, dt_avg_rec)
sprintf('랜덤 포레스트: 정확도=%f, 정밀도=%f, 재현율=%f',
rf_avg_acc, rf_avg_prec, rf_avg_rec)
sprintf('SVM: 정확도=%f, 정밀도=%f, 재현율=%f',
sv_avg_acc, sv_avg_prec, sv_avg_rec)
sprintf('K-NN: 정확도=%f, 정밀도=%f, 재현율=%f',
kn_avg_acc, kn_avg_prec, kn_avg_rec)
# UCLA 데이터에 대해서 Cross validation을 하고
# 4가지 모델에 대해서 정확도, 정밀도, 재현율을 구하시오.
library(caret)
library(rpart)
library(randomForest)
library(e1071)
library(class)
ucla <- read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv')
ucla$admit <- factor(ucla$admit)
set.seed(2021)
data <- ucla[sample(nrow(ucla)),]
options(digits = 4)
# K-Fold CV, K=5
k <- 5
q <- nrow(data) / k
l <- 1:nrow(data)
# Decision Tree
accuracy <- 0
precision <- 0
recall <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
dt <- rpart(admit~., data_train)
pred <- predict(dt, data_test, type='class')
t <- table(pred, data_test$admit)
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
precision <- precision + t[2,2]/(t[2,1]+t[2,2])
recall <- recall + t[2,2]/(t[1,2]+t[2,2])
}
dt_avg_acc <- accuracy / k
dt_avg_prec <- precision / k
dt_avg_rec <- recall / k
# Random Forest
accuracy <- 0
precision <- 0
recall <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(admit~., data_train)
pred <- predict(rf, data_test, type='class')
t <- table(pred, data_test$admit)
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
precision <- precision + t[2,2]/(t[2,1]+t[2,2])
recall <- recall + t[2,2]/(t[1,2]+t[2,2])
}
rf_avg_acc <- accuracy / k
rf_avg_prec <- precision / k
rf_avg_rec <- recall / k
# Support Vector Machine
accuracy <- 0
precision <- 0
recall <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
sv <- svm(admit~., data_train)
pred <- predict(sv, data_test, type='class')
t <- table(pred, data_test$admit)
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
precision <- precision + t[2,2]/(t[2,1]+t[2,2])
recall <- recall + t[2,2]/(t[1,2]+t[2,2])
}
sv_avg_acc <- accuracy / k
sv_avg_prec <- precision / k
sv_avg_rec <- recall / k
# K-Nearest Neighbor
accuracy <- 0
precision <- 0
recall <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
pred <- knn(data_train[, 2:4], data_test[, 2:4],
data_train$admit, k=5)
t <- table(pred, data_test$admit)
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
precision <- precision + t[2,2]/(t[2,1]+t[2,2])
recall <- recall + t[2,2]/(t[1,2]+t[2,2])
}
kn_avg_acc <- accuracy / k
kn_avg_prec <- precision / k
kn_avg_rec <- recall / k
sprintf('결정트리: 정확도=%f, 정밀도=%f, 재현율=%f',
dt_avg_acc, dt_avg_prec, dt_avg_rec)
sprintf('랜덤 포레스트: 정확도=%f, 정밀도=%f, 재현율=%f',
rf_avg_acc, rf_avg_prec, rf_avg_rec)
sprintf('SVM: 정확도=%f, 정밀도=%f, 재현율=%f',
sv_avg_acc, sv_avg_prec, sv_avg_rec)
sprintf('K-NN: 정확도=%f, 정밀도=%f, 재현율=%f',
kn_avg_acc, kn_avg_prec, kn_avg_rec)
# 1, colon 데이터에 랜덤 포리스트를 적용하는데, k-겹 교차 검증을 k를 5,10,15,20으로 바꾸면서 적용하라.
# 각각의 혼동 행렬과 정확률을 제시하라.
library(survival)
library(randomForest)
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
for (k in seq(5,20,5)) {
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
sprintf('k = %d 일때의 혼동행렬, 정확률 = %f',k,accuracy )
}
t
t, accuracy
t
accuracy
for (k in seq(5,20,5)) {
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
}
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
# 1, colon 데이터에 랜덤 포리스트를 적용하는데, k-겹 교차 검증을 k를 5,10,15,20으로 바꾸면서 적용하라.
# 각각의 혼동 행렬과 정확률을 제시하라.
library(survival)
library(randomForest)
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
k <- 5
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
k <- 10
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
k <- 15
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
k <- 20
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
options(digits = 4)
head(data)
clean_colon
tail(colon)
control <- trainControl(method='cv', number=5)
new_rf <- train(Species~., data=iris, method='rf',
metric='Accuracy', trControl=control)
confusionMatrix(new_rf)
###############################
# 4개 모델에 적용
###############################
control <- trainControl(method='cv', number=5)
dt <- train(Species~., data=iris, method='rpart',
metric='Accuracy', trControl=control)
rf <- train(Species~., data=iris, method='rf',
metric='Accuracy', trControl=control)
sv <- train(Species~., data=iris, method='svmRadial',
metric='Accuracy', trControl=control)
kn <- train(Species~., data=iris, method='knn',
metric='Accuracy', trControl=control)
resamp <- resamples(list(결정트리=dt, 랜덤포레스트=rf,
SVM=sv, KNN=kn))
summary(resamp)
sort(resamp, decreasing=T)
# K-Fold 교차 검증(Cross Validation)
library(caret)
###############################
# 4개 모델에 적용
###############################
control <- trainControl(method='cv', number=5)
dt <- train(Species~., data=iris, method='rpart',
metric='Accuracy', trControl=control)
rf <- train(Species~., data=iris, method='rf',
metric='Accuracy', trControl=control)
sv <- train(Species~., data=iris, method='svmRadial',
metric='Accuracy', trControl=control)
kn <- train(Species~., data=iris, method='knn',
metric='Accuracy', trControl=control)
resamp <- resamples(list(결정트리=dt, 랜덤포레스트=rf,
SVM=sv, KNN=kn))
summary(resamp)
sort(resamp, decreasing=T)
ucla
library(ucla)
ucla <- read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv')
ucla$admit <- factor(ucla$admit)
control <- trainControl(method='cv', number=5)
dt <- train(Species~., data=ucla, method='rpart',
metric='Accuracy', trControl=control)
rf <- train(Species~., data=iris, method='rf',
metric='Accuracy', trControl=control)
sv <- train(Species~., data=iris, method='svmRadial',
metric='Accuracy', trControl=control)
kn <- train(Species~., data=iris, method='knn',
metric='Accuracy', trControl=control)
ucla <- read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv')
ucla$admit <- factor(ucla$admit)
control <- trainControl(method='cv', number=5)
dt <- train(admit~., data=ucla, method='rpart',
metric='Accuracy', trControl=control)
rf <- train(admit~., data=ucla, method='rf',
metric='Accuracy', trControl=control)
sv <- train(admit~., data=ucla, method='svmRadial',
metric='Accuracy', trControl=control)
kn <- train(admit~., data=ucla, method='knn',
metric='Accuracy', trControl=control)
resamp <- resamples(list(결정트리=dt, 랜덤포레스트=rf,
SVM=sv, KNN=kn))
summary(resamp)
sort(resamp, decreasing=T)
control <- trainControl(method='cv', number=5)
dt <- train(admit~., data=ucla, method='rpart',
metric='Accuracy', trControl=control)
rf <- train(admit~., data=ucla, method='rf',
metric='Accuracy', trControl=control)
sv <- train(admit~., data=ucla, method='svmRadial',
metric='Accuracy', trControl=control)
kn <- train(admit~., data=ucla, method='knn',
metric='Accuracy', trControl=control)
resamp <- resamples(list(결정트리=dt, 랜덤포레스트=rf,
SVM=sv, KNN=kn))
summary(resamp)
sort(resamp, decreasing=T)
knitr::opts_chunk$set(echo = TRUE)
library(survival)
library(randomForest)
getwd()
setwd("C:/workspace/r")
getwd()
# 2. voice 데이터에 대해 수행
getwd()
ucla <- read.csv('https://stats.idre.ucla.edu/stat/data/binary.csv', header = TRUE)
head(voice)
voice <- read.csv("C:/workspace/r/data/voice.csv", header = TRUE)
head(voice)
str(voice)
voice$label <- factor(voice$label)
str(voice)
head(voice)
voice <- read.csv("C:/workspace/r/data/voice.csv", header = TRUE)
voice$label <- factor(voice$label)
str(voice)
control <- trainControl(method='cv', number=5)
dt <- train(label~., data=voice, method='rpart',
metric='Accuracy', trControl=control)
rf <- train(label~., data=voice, method='rf',
metric='Accuracy', trControl=control)
sv <- train(label~., data=voice, method='svmRadial',
metric='Accuracy', trControl=control)
kn <- train(label~., data=voice, method='knn',
metric='Accuracy', trControl=control)
resamp <- resamples(list(결정트리=dt, 랜덤포레스트=rf,
SVM=sv, KNN=kn))
summary(resamp)
sort(resamp, decreasing=T)
summary(resamp)
sort(resamp, decreasing=T)
library(survival)
library(randomForest)
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
options(digits = 4)
for(k in seq(5,20,5)) {
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
}
library(survival)
library(randomForest)
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
options(digits = 4)
for(k in 5*c(1:4)) {
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
}
library(survival)
library(randomForest)
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
options(digits = 4)
for(k in 5*c(1:4)) {
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
}
library(survival)
library(randomForest)
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
options(digits = 4)
for(a in seq(5,20,5)) {
k <- a
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
}
library(survival)
library(randomForest)
library(survival)
library(randomForest)
head(colon)
set.seed(2021)
clean_colon <- na.omit(colon)
data <- clean_colon[sample(nrow(clean_colon)),]
data$status <- as.factor(data$status)
options(digits = 4)
for(a in seq(5,20,5)) {
k <- a
q <- nrow(data) / k
l <- 1:nrow(data)
accuracy <- 0
for (i in 1:k) {
test_list <- ((i-1)*q+1) : (i*q)
data_test <- data[test_list,]
data_train <- data[-test_list,]
rf <- randomForest(status~., data_train)
pred <- predict(rf, data_test, type = 'class')
t <- table(pred, data_test$status)
}
accuracy <- accuracy + (t[1,1]+t[2,2])/nrow(data_test)
t
accuracy
}
str(i)
i/5
str(i/5)
as.integer(i/5)
}install.packages('rjava')
install.packages('rJava')
install.packages('memoise’)
install.packages('memoise’)
install.packages('memoise’)
install.packages('memoise')
install.packages('memoise')
install.packages('hash')
install.packages('tau')
install.packages('Sejong')
install.packages('devtools')
install.packages('RSQLite')
install.packages("https://cran.r-project.org/src/contrib/Archive/KoNLP/KoNLP_0.80.2.tar.gz", repos = NULL, type="source", INSTALL_opts = c('--no-lock'))
.libPaths()
install.packages("https://cran.r-project.org/src/contrib/Archive/KoNLP/KoNLP_0.80.2.tar.gz", repos = NULL, type="source", INSTALL_opts = c('--no-lock'))
